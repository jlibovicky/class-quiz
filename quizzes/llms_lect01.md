# Large Language Models: Class 1 Recap

## What do language models predict?  
1. (X) A probability distribution of what the next word might be
2. What the next word is, but there is no probabilistic interpretation
3. The embedding vector of the entire sentence before it is written
4. Binary probability is a continuation of a sentence is authentic or machine-generated language

## Which of the following is **NOT** a reason a language model is considered "large"?  
1. It has been trained on vast amounts of data
2. It has billions of parameters
3. It exhibits emergent properties at scale
4. (X) It has so many parameters that it does improve with more data

## What kind of data is used to train large language models?  
1. Only peer-reviewed academic papers to ensure factual accuracy
2. (X) Text from books, articles, conversations, and website
3. Manually curated sentences written by linguists
4. A small set of pre-defined grammar rules

## What is the primary training objective of a large language model?  
1. (X) To maximize the probability of training data given the model
2. To memorize the training data so the model quickly retrieve facts
3. To minimize the entropy in the attention mechanism
4. To minimize the cross-entropy of the attention mechanism and the training data
