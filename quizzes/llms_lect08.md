# Large Language Models: Lecture 8 Quiz

## What is usually the most in-demand computational resource for fine-tuning LLM?
1. (X) Disk space
2. VRAM (GPU Memory)
3. Inference time
4. RAM (CPU Memory)

## How much GPU memory is approximately needed to fine-tune 10B parameter model (with Adam optimizer)?
1. 16 GB
2. 61 GB
3. (X) 160 GB
4. 610 GB

## LoRA is an algorithm for:
1. (X) Parameter efficient fine-tuning
2. Speeding-up inference
3. Quantization
4. Data compression

## Which of the following statements is *false*?
1. RAG can retrieve additional information using a web search engine (e.g., Google).
2. (X) RAG eliminates hallucinations by forcing the model to cite the answer sources.
3. RAG can be implemented over a private knowledge base.
4. The results that RAG retrieves are influenced by the content of the user prompt.

## What is the principle of test-time scaling?
1. You can get more precise evaluation results by testing a model on multiple benchmarks.
2. You should deploy large-scale models only along with the appropriate test suite.
3. You need to use large-scale models to pass standardized examination tests.
4. (X) You can achieve better results by using more computational time during model inference.
